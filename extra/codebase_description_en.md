# Codebase Description

This document provides a detailed description of each file and folder in the codebase, with references to the relevant steps in the paper "Reformulating Unsupervised Style Transfer as Paraphrase Generation".

## Top-Level Files and Folders

*   `.gitignore`: Specifies intentionally untracked files that Git should ignore.
*   `demo_paraphraser.py`: Implements a command-line demo for paraphrasing single sentences using a pre-trained GPT-2 model (Section 3 in the paper).
*   `description_en.md`: This file (the English description of the codebase).
*   `description_vi.md`: The Vietnamese description of the codebase.
*   `LICENSE`: Contains the license information for the codebase.
*   `paraphrase_many.py`: Implements a script for paraphrasing multiple sentences from an input file and writing the output to another file (Section 3 in the paper).
*   `README_terminal_demo.md`: Provides instructions on how to run the command-line demo.
*   `README-multilingual.md`: Provides details about multilingual classifiers for formality evaluation.
*   `README.md`: Provides a general overview of the project, including setup instructions, dataset information, and training/evaluation procedures.
*   `requirements.txt`: Lists the Python packages required to run the codebase.
*   `setup.py`: Defines the package metadata and install options.
*   `data_samples/`: Contains sample data files for different styles (e.g., Shakespeare, Tweets, Bible).
*   `datasets/`: Contains scripts for preparing datasets for training and evaluation (Section 4 in the paper).
*   `mturk_evals/`: Contains data and scripts for Mechanical Turk evaluations (Section 3 in the paper).
*   `outputs/`: Contains the outputs generated by the model and baselines.
*   `papers/`: Contains the research paper and slides.
*   `style_paraphrase/`: Contains the core code for the style transfer model, including the diverse paraphraser, inverse paraphrase models, and training/evaluation scripts.
*   `web-demo/`: Contains the code for the web demo.

## Detailed Description of Key Folders and Files

### `data_samples/`

*   `aae.txt`: Sample data in African American English style.
*   `bible.txt`: Sample data in Biblical style.
*   `coha_1810-1830.txt`: Sample data from the Corpus of Historical American English (1810-1830).
*   `coha_1890-1910.txt`: Sample data from the Corpus of Historical American English (1890-1910).
*   `coha_1990-2000.txt`: Sample data from the Corpus of Historical American English (1990-2000).
*   `english_tweets.txt`: Sample data in English Tweet style.
*   `joyce.txt`: Sample data from James Joyce's works.
*   `lyrics.txt`: Sample data in Lyrics style.
*   `README.md`: Describes the contents of the `data_samples` directory.
*   `romantic_poetry.txt`: Sample data in Romantic Poetry style.
*   `shakespeare.txt`: Sample data in Shakespearean style.
*   `switchboard.txt`: Sample data from the Switchboard corpus (conversational speech).

### `datasets/`

*   `bpe2text.py`: Converts a BPE file back into its raw text form (used in data preprocessing).
*   `dataset_config.py`: Contains configuration settings for different datasets.
*   `prepare_paraphrase_data.py`: Preprocesses a TSV data of sentence pairs to a compatible format for paraphrase training (Section 2.1 in the paper).
*   `style_dataset.py`: Defines the `ParaphraseDatasetText` and `InverseParaphraseDatasetText` classes, which are used to load and process the data for training the paraphrase model and inverse paraphrase models, respectively.

### `mturk_evals/`

This directory contains data and results from Mechanical Turk evaluations used to assess the quality of the style transfer models (Section 3 in the paper).

### `outputs/`

This directory contains the outputs generated by the model and baselines.

### `papers/`

*   `EMNLP 2020 Slides.pdf`: Contains the slides presented at EMNLP 2020.
*   `Reformulating Unsupervised Style Transfer as Paraphase Generation.pdf`: Contains the research paper.

### `style_paraphrase/`

*   `__init__.py`: An empty file that indicates that the `style_paraphrase` directory is a Python package.
*   `args.py`: Defines the command-line arguments for the training and evaluation scripts.
*   `data_utils.py`: Contains utility functions for loading, preprocessing, and batching the data (Section 2.1 and 2.2 in the paper).
*   `dataset_config.py`: Contains configuration settings for different datasets.
*   `hyperparameters_config.py`: Defines the hyperparameters for the training process.
*   `inference_utils.py`: Contains the `GPT2Generator` class, which is used to generate paraphrases and perform style transfer (Section 2.3 in the paper).
*   `run_evaluate_gpt2_template.sh`: A template script for evaluating the GPT-2 model.
*   `run_finetune_gpt2_template.sh`: A template script for fine-tuning the GPT-2 model.
*   `run_lm_finetuning.py`: Implements the language model fine-tuning process (Section 2.3 in the paper).
*   `schedule.py`: Contains code for scheduling and running experiments on a SLURM cluster.
*   `style_dataset.py`: Defines the `ParaphraseDatasetText` and `InverseParaphraseDatasetText` classes, which are used to load and process the data for training the paraphrase model and inverse paraphrase models, respectively (Section 2.1 and 2.2 in the paper).
*   `utils.py`: Contains utility functions for initializing the GPT-2 model, sampling sequences, and performing beam search (Section 2.3 in the paper).
*   `style_classify/`: Contains code for training and evaluating style classifiers (Section 3 in the paper).
*   `evaluation/`: Contains code for evaluating the style transfer models (Section 3 in the paper).
*   `examples/`: Contains example scripts for running the training and evaluation processes.

### `web-demo/`

*   `clean_queue.py`: A script likely used to clear the processing queue for the web demo.
*   `config.json`: Contains configuration settings for the web demo, such as model paths and API endpoints.
*   `demo_service.py`: Implements the backend logic for the web demo, handling requests and generating paraphrases.
*   `LICENSE`: Contains the license information for the web demo.
*   `README.md`: Provides instructions on how to set up and run the web demo.
*   `setup.sh`: A shell script that likely installs the necessary dependencies and sets up the environment for the web demo.
*   `strap-backend/`: Contains the backend code for the web demo.
    *   `app.py`: Implements the Flask application that serves the web demo.
    *   `waitress_server.py`: Likely used to run the Flask application using the Waitress WSGI server.
*   `strap-frontend/`: Contains the frontend code for the web demo.
    *   `.gitignore`: Specifies intentionally untracked files that Git should ignore.
    *   `package-lock.json`: Records the exact versions of the dependencies used in the frontend project.
    *   `package.json`: Defines the dependencies and scripts for the frontend project.
    *   `README.md`: Provides instructions on how to set up and run the frontend.
    *   `public/`: Contains static assets for the frontend, such as HTML, CSS, and JavaScript files.
*   `strap-landing/`: Likely contains code for a landing page for the web demo.

## Key Files and Their Relation to the STRAP Method

*   `style_paraphrase/inference_utils.py`: This file contains the `GPT2Generator` class, which is the core component of the STRAP method. It implements the diverse paraphrasing and style transfer processes (Section 2.3 in the paper).
*   `style_paraphrase/run_lm_finetuning.py`: This file implements the fine-tuning process for the inverse paraphrase models (Section 2.3 in the paper).
*   `style_paraphrase/utils.py`: This file contains utility functions for initializing the GPT-2 model, sampling sequences, and performing beam search (Section 2.3 in the paper).
*   `datasets/prepare_paraphrase_data.py`: This file implements the data filtering process to promote diversity in the paraphrase model (Section 2.1 in the paper).
